---
title: "Group 1 Parkinsons Data Set"
format: html
editor: visual
---

## Group 1

Tara Dehdari, Patricio Martinez, Duy Nguyen

## Problem Statement:

The goal is to develop a robust predictive model to accurately predict the progression of Parkinson's disease as measured by the Unified Parkinson's Disease Rating Scale (UPDRS) based on telemonitoring data, and to dientify the most influential features contributing to the prediction. 

#### Load Libraries

```{r}
library(caret)
library(randomForest)
library(tidyverse)
library(ggplot2)
library(earth)
library(kernlab)
library(dplyr)
library(corrplot)
library(Metrics)
theme_set(theme_minimal())

#Set seed to reproduce samples
seed <- 123
```

#### Load Data

```{r}
df <- read.csv("parkinsons_updrs.data.csv")
head(df)

```

## Exploratory Data Analysis:

#### Data Types

```{r}
str(df)
```
```{r}
summary(df)
```
```{r}
dim(df)
```

#### Missing Values

```{r}
sum(is.na(df))
```

#### Visualizations

```{r}
# Age distribution
ggplot(df, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution of Patients", x = "Age", y = "Count")

# Total UPDRS distribution
ggplot(df, aes(x = total_UPDRS)) +
  geom_histogram(binwidth = 5, fill = "green", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Total UPDRS Scores", x = "Total UPDRS", y = "Count")

# Age vs. Total UPDRS
ggplot(df, aes(x = age, y = total_UPDRS)) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Age vs. Total UPDRS", x = "Age", y = "Total UPDRS")
```
```{r}
# Count the number of males and females
sex_count <- df %>% 
  group_by(sex) %>%
  summarise(count = n())

# Convert 'sex' to factor for better labeling
sex_count$sex <- factor(sex_count$sex, labels = c("Female", "Male"))

# Create a bar plot for the distribution of males and females
ggplot(sex_count, aes(x = sex, y = count, fill = sex)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Distribution of Males and Females", x = "Sex", y = "Count") +
  scale_fill_manual(values = c("Female" = "pink", "Male" = "blue"))
```
## Preprocessing

#### Normalize the Dataset

```{r}
# gather numeric columns
numeric_cols <- sapply(df, is.numeric)

# normalizing the numerical features
df[numeric_cols] <- lapply(df[numeric_cols], scale)
```

#### Resampling Female and Male

```{r}
# determine the minimum count
min_count <- min(sex_count$count)

# downsample the majority class 
df_downsample <- df %>%
  group_by(sex) %>%
  sample_n(min_count)

# Visualizing new amount of male and female

# Count the number of males and females after downsampling
sex_count_downsampled <- df_downsample %>%
  group_by(sex) %>%
  summarise(count = n())

# Convert 'sex' to factor for better labeling
sex_count_downsampled$sex <- factor(sex_count_downsampled$sex, labels = c("Female", "Male"))

# Create a bar plot for the distribution of males and females after downsampling
ggplot(sex_count_downsampled, aes(x = sex, y = count, fill = sex)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Distribution of Males and Females after Downsampling", x = "Sex", y = "Count") +
  scale_fill_manual(values = c("Female" = "pink", "Male" = "blue"))
```

#### Heatmap Correlation

```{r}
correlations <- cor(df)

corrplot(correlations, order = "hclust")

print(correlations)
```
##### Removing High Correlations

```{r}

# find highly correlated predictors
highCorr <- findCorrelation(correlations, cutoff = 0.80)
cat("The amount of high correlations:", length(highCorr), "\n")
cat("\n")
cat ("Values with high correlations: \n")
cat(highCorr)

```
```{r}
# removing high correlations
filtered_df <- df[,-highCorr]

# correlation heatmap again 
correlationsFiltered <- cor(filtered_df)
corrplot(correlationsFiltered, order = "hclust")
```
```{r}
# Print dimensions of filtered dataset and original dataset
cat("Original data dimensions:", dim(df), "\n")
cat("Filtered data dimensions:", dim(filtered_df), "\n")
```

#### Outlier Check

Calculating what would be considered an outlier shows that there are quiet a few but looking at the boxplots nothing is significant enought to withdraw from the dataset. 

```{r}
# Boxplot for each numerical variable
boxplot(df[,numeric_cols], outline=TRUE, col="lightblue", main="Boxplot of Numerical Variables")

# Function to detect outliers using IQR method
detect_outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  outliers <- x < lower_bound | x > upper_bound
  return(outliers)
}

# Apply the function to each numerical variable
outliers <- apply(df[,numeric_cols], 2, detect_outliers)

# Count the number of outliers for each variable
outlier_counts <- colSums(outliers)
outlier_counts
```

#### Visualizing Target Variable (total_UPDRS (Unified Parkinson's Disease Rating Scale Score))

```{r}
# Visualize the distribution of total_UPDRS with density plot
ggplot(filtered_df, aes(x = total_UPDRS)) +
  geom_density(fill = "green", color = "black") +
  theme_minimal() +
  labs(title = "Density Plot of Total UPDRS Scores", x = "Total UPDRS", y = "Density")

```
#### Relationships between target and predictor variables

```{r}
# Select predictor variables (all columns except total_UPDRS)
predictor_vars <- filtered_df[, !colnames(filtered_df) %in% "total_UPDRS"]

# Create scatter plots for each predictor variable against total_UPDRS
for (var in colnames(predictor_vars)) {
  p <- ggplot(filtered_df, aes_string(x = var, y = "total_UPDRS")) +
    geom_point(alpha = 0.5) +
    theme_minimal() +
    labs(title = paste("Scatter Plot of", var, "vs. Total UPDRS"), 
         x = var, y = "Total UPDRS")
  
  print(p)
}
```


## Data Splitting:

```{r}
# Split the data into training and test sets (80% train, 20% test)
set.seed(seed)
trainIndex <- createDataPartition(filtered_df$total_UPDRS, p = 0.8, list = FALSE)
df_train <- filtered_df[trainIndex, ]
df_test <- filtered_df[-trainIndex, ]

# Print dimensions of training and test sets
cat("Training data dimensions:", dim(df_train), "\n")
cat("Test data dimensions:", dim(df_test), "\n")

```

## Model Building:

#### Linear Regression

```{r}
# train linear moddel
linear_model <- lm(total_UPDRS ~ .,
                   data = df_train)

linear_model

# predict linear model
linear_predictions <- predict(linear_model, 
                              df_test)

```

#### Random Forest

```{r}
# set seed for reproducibiltiy
set.seed(seed)

# train random forest model
rf_model <- randomForest(total_UPDRS ~ .,
                         data = df_train,
                         importance = TRUE,
                         ntree = 500)
rf_model

# predict random forest model
rf_predictions <- predict(rf_model, df_test)
```


#### Support Vector Machine

```{r}
# set seed for reproducibiltiy
set.seed(seed)

# train SVM model
svm_model <- ksvm(total_UPDRS ~ ., 
                  data = df_train, 
                  kernel = "rbfdot")
svm_model

# predict SVM model
svm_predictions <- predict(svm_model, 
                           df_test)
```


#### MARS

```{r}
# set seed for reproducibiltiy
set.seed(seed)

# train MARS model
mars_model <- earth(total_UPDRS ~ ., data = df_train)

mars_model

# predict MARS model
mars_predictions <- predict(mars_model, df_test)
```


#### Gradient Boosting Machine

```{r}
# set seed for reproducibiltiy
set.seed(seed)

# train GBM model
gbm_model <- train(total_UPDRS ~ ., 
                   data = df_train, 
                   method = "gbm", 
                   trControl = trainControl(method = "cv", number = 5))
gbm_model

# predict GBM model
gbm_predictions <- predict(gbm_model, df_test)
```


#### Neural Network

```{r}
# set seed for reproducibiltiy
set.seed(seed)

# train NNET model
nn_model <- train(total_UPDRS ~ ., 
                  data = df_train, 
                  method = "nnet", 
                  trControl = trainControl(method = "cv", number = 5), 
                  linout = TRUE, trace = FALSE)
nn_model

# predict NNET model
nn_predictions <- predict(nn_model, df_test)
```


## Evaluation Metrics

```{r}
# Initialize empty dataframe to store results
testResults <- data.frame(Model = character(),
                          RMSE = numeric(),
                          MAE = numeric(),
                          Rsquared = numeric(),
                          stringsAsFactors = FALSE)

# Function to calculate evaluation metrics
evaluate_model <- function(true_values, predictions) {
  rmse <- sqrt(mean((predictions - true_values)^2))
  mae <- mean(abs(predictions - true_values))
  rsquared <- 1 - sum((predictions - true_values)^2) / sum((true_values - mean(true_values))^2)
  return(c(RMSE = rmse, MAE = mae, Rsquared = rsquared))
}
```

#### Calculate Evaluation Metrics for Each Model

```{r}
# evaluation metrics for linear regression
linear_metrics <- evaluate_model(df_test$total_UPDRS, linear_predictions)

# evaluation metrics for random forest 
rf_metrics <- evaluate_model(df_test$total_UPDRS, rf_predictions)

# evaluation metrics for support vector machine
svm_metrics <- evaluate_model(df_test$total_UPDRS, svm_predictions)

# evaluation metrics for MARS
mars_metrics <- evaluate_model(df_test$total_UPDRS, mars_predictions)

# evaluation metrics for Gradient Boosting Machine
gbm_metrics <- evaluate_model(df_test$total_UPDRS, gbm_predictions)

# evaluation metrics for neural network
nn_metrics <- evaluate_model(df_test$total_UPDRS, nn_predictions)
```

#### Store Evaluation Results in DataFrame

```{r}
# store results for linear regression
testResults <- rbind(testResults, data.frame(Model = "Linear Regression", 
                                             RMSE = linear_metrics["RMSE"], 
                                             MAE = linear_metrics["MAE"], 
                                             Rsquared = linear_metrics["Rsquared"]))


# store results for random forest 
testResults <- rbind(testResults, data.frame(Model = "Random Forest", 
                                             RMSE = rf_metrics["RMSE"], 
                                             MAE = rf_metrics["MAE"], 
                                             Rsquared = rf_metrics["Rsquared"]))

# store results for support vector machine
testResults <- rbind(testResults, data.frame(Model = "Support Vector Machine", 
                                             RMSE = svm_metrics["RMSE"], 
                                             MAE = svm_metrics["MAE"], 
                                             Rsquared = svm_metrics["Rsquared"]))


# store results for MARS
testResults <- rbind(testResults, data.frame(Model = "MARS", 
                                             RMSE = mars_metrics["RMSE"], 
                                             MAE = mars_metrics["MAE"], 
                                             Rsquared = mars_metrics["Rsquared"]))


# store results for Gradient Boosting Machine
testResults <- rbind(testResults, data.frame(Model = "GBM", 
                                             RMSE = gbm_metrics["RMSE"], 
                                             MAE = gbm_metrics["MAE"], 
                                             Rsquared = gbm_metrics["Rsquared"]))


# store results for neural network
testResults <- rbind(testResults, data.frame(Model = "Neural Network", 
                                             RMSE = nn_metrics["RMSE"], 
                                             MAE = nn_metrics["MAE"], 
                                             Rsquared = nn_metrics["Rsquared"]))

# print results
print(testResults)

```


## Results of Model Building

RMSE measures the square root of the average squared differences between the predicted and actual values, so lower values are better. MAE measures the average absolute differences between the predicted and actual values, so lower values are also better. R-squared indicates how well the independent variables explain the variability of the dependent variable. Higher values are better (closer to 1). 
Based off of these Random Forest would be our best model. It has the lowers RMSE and MAE and the highest R-squared. This indicates that Random Forest model performs the best in predicting the 'total_UPDRS' scores with the highest accuracy and lowest prediction error. GBM also performs well at second best but still not as good as random forests model. 

Our recommendation is to use the Random Forest model for predicting the progression of Parkinson's diseases as it provides the best model based on the evaluation metrics. 

## Hyperparameter Tuning

```{r}
# set up train control for cross-validation
set.seed(seed)
ctrl <- trainControl(method = "cv", number = 5)

# define the grid of hyperparameters to search
tune_grid <- expand.grid(mtry = c(2, 3, 6, 8, 10, 12, 13, 15))

# Perform hyperparameter tuning
set.seed(123)
rf_tuned <- train(total_UPDRS ~ ., data = df_train, 
                  method = "rf", 
                  trControl = ctrl, 
                  tuneGrid = tune_grid)

# Print the best parameters
print(rf_tuned$bestTune)

# Plot the results of hyperparameter tuning
plot(rf_tuned)
```
```{r}
# Predict on test data using the tuned model
rf_tuned_predictions <- predict(rf_tuned, df_test)

# Evaluate the tuned model
evaluate_model <- function(true_values, predictions) {
  rmse <- sqrt(mean((predictions - true_values)^2))
  mae <- mean(abs(predictions - true_values))
  rsquared <- 1 - sum((predictions - true_values)^2) / sum((true_values - mean(true_values))^2)
  return(c(RMSE = rmse, MAE = mae, Rsquared = rsquared))
}

rf_tuned_metrics <- evaluate_model(df_test$total_UPDRS, rf_tuned_predictions)

# Print the evaluation metrics for the tuned model
rf_tuned_metrics
```

## Feature Importance

```{r}
# Extract feature importance from the tuned Random Forest model
rf_importance <- varImp(rf_tuned, scale = FALSE)

# Print the feature importance
print(rf_importance)

# Plot the feature importance
plot(rf_importance, top = 10, main = "Top 10 Feature Importance - Random Forest")
```

## Final Results

Based on the hyper parameter tuning, the best model configuration for the Random Forest was found with 'mtry= 13', yielding an RMSE of  0.0321, and MAE of 0.0134, and an R-squared of 0.9990, indicating excellent predictive performance. The most influential features in predicting the Unified Parkinson's Disease Rating Scale (UPDRS) scores were subject, age, test time, sex and RPDE. These results suggest that the model can accurately predict disease progression using telemonitoring data, with the mentioned features plating a crucial role. 

